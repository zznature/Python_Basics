{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How the backpropagation algorithm works\n",
    "\n",
    "**Goals**\n",
    "\n",
    "  * multi-varialbe caculus (chain rule)\n",
    "  * interpretion of codes\n",
    "\n",
    "First, what’s the algorithm really doing? We’ve developed a picture of the error being backpropagated from the output. But can we go any deeper, and build up more intuition about what is going on when we do all these matrix and vector multiplications? \n",
    "\n",
    "The second mystery is how someone could ever have discovered backpropagation in the first place?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The backpropagation algorithm\n",
    "\n",
    "\n",
    "### Math Interpretation\n",
    "The backpropagation equations provide us with a way of computing the gradient of the cost function. Let's explicitly write this out in the form of an algorithm:\n",
    "\n",
    "1. Input $x$ : Set the corresponding activation $a^{1}$ for the input layer.\n",
    "2. Feedforward: For each $l=2,3, \\ldots, L$ compute $z^{l}=w^{l} a^{l-1}+b^{l}$ and $a^{l}=\\sigma\\left(z^{l}\\right) .$\n",
    "3. Output error $\\delta^{L}$ : Compute the vector $\\delta^{L}=\\nabla_{a} C \\odot \\sigma^{\\prime}\\left(z^{L}\\right)$.\n",
    "4. Backpropagate the error: For each $l=L-1, L-2, \\ldots, 2$ compute $\\delta^{l}=\\left(\\left(w^{l+1}\\right)^{T} \\delta^{l+1}\\right) \\odot \\sigma^{\\prime}\\left(z^{l}\\right)$.\n",
    "5. Output: The gradient of the cost function is given by $\\frac{\\partial C}{\\partial w_{j k}^{l}}=a_{k}^{l-1} \\delta_{j}^{l}$ and $\\frac{\\partial C}{\\partial b_{j}^{l}}=\\delta_{j}^{l} .$\n",
    "\n",
    "\n",
    "Examining the algorithm you can see why it's called backpropagation. We compute the error vectors δl backward, starting from the final layer. It may seem peculiar that we're going through the network backward. But if you think about the proof of backpropagation, the backward movement is a consequence of the fact that the cost is a function of outputs from the network. To understand how the cost varies with earlier weights and biases we need to repeatedly apply the chain rule, working backward through the layers to obtain usable expressions.\n",
    "\n",
    "**Exercises**\n",
    "\n",
    "Backpropagation with a single modified neuron Suppose we modify a single neuron in a feedforward network so that the output from the neuron is given by f(∑jwjxj+b), where f is some function other than the sigmoid. How should we modify the backpropagation algorithm in this case?\n",
    "\n",
    "\n",
    "### Programming Interpretation\n",
    "As I've described it above, the backpropagation algorithm computes the gradient of the cost function for a single training example, $C=C_{x}$. In practice, it's common to combine backpropagation with a learning algorithm such as stochastic gradient descent, in which we compute the gradient for many training examples. In particular, given a mini-batch of $m$ training examples, the following algorithm applies a gradient descent learning step based on that mini-batch:\n",
    "1. Input a set of training examples\n",
    "2. For each training example $x$ : Set the corresponding input activation $a^{x, 1}$, and perform the following steps:\n",
    "   \n",
    "  * Feedforward: For each $l=2,3, \\ldots, L$ compute $z^{x, l}=w^{l} a^{x, l-1}+b^{l}$ and $a^{x, l}=\\sigma\\left(z^{x, l}\\right) .$\n",
    "  * Output error $\\delta^{x, L}$ : Compute the vector $\\delta^{x, L}=\\nabla_{a} C_{x} \\odot \\sigma^{\\prime}\\left(z^{x, L}\\right) .$\n",
    "\n",
    "  * Backpropagate the error: For each\n",
    "$l=L-1, L-2, \\ldots, 2$ compute\n",
    "$\\delta^{x, l}=\\left(\\left(w^{l+1}\\right)^{T} \\delta^{x, l+1}\\right) \\odot \\sigma^{\\prime}\\left(z^{x, l}\\right)$.\n",
    "3. Gradient descent: For each $l=L, L-1, \\ldots, 2$ update the weights according to the rule $w^{l} \\rightarrow w^{l}-\\frac{\\eta}{m} \\sum_{x} \\delta^{x, l}\\left(a^{x, l-1}\\right)^{T}$, and the biases according to the rule $b^{l} \\rightarrow b^{l}-\\frac{\\eta}{m} \\sum_{x} \\delta^{x, l}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code for backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "\n",
    "\n",
    "   def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple \"(nabla_b, nabla_w)\" representing the\n",
    "        gradient for the cost function C_x.  \"nabla_b\" and\n",
    "        \"nabla_w\" are layer-by-layer lists of numpy arrays, similar\n",
    "        to \"self.biases\" and \"self.weights\".\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y) \n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Backpropagation: the big picture"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8fdff399fcb569556feca9492b2b24eb7759cd3bb1b439d9fa03398f230501f8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('python3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
